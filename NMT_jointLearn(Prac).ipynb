{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_jointLearn(Prac).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yinghao1019/NLP_and_DL_practice/blob/master/NMT_jointLearn(Prac).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrPOfjV-7kvU"
      },
      "source": [
        "Attention Based Sequence to Sequence Model.(German-english)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76rBGCSqhbhZ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import time\n",
        "import tqdm\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqJpZGkXlHnd"
      },
      "source": [
        "#set device\n",
        "device=torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "#load German & english spacy tokenizer\n",
        "de_nlp=spacy.load('de')\n",
        "en_nlp=spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skkOOLGdoLPc"
      },
      "source": [
        "Build text *tokenizer* func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkIrbnUpoQZN"
      },
      "source": [
        "def en_tokenizer(text):\n",
        "  return [t.text for t in en_nlp.tokenizer(text)]\n",
        "def de_tokenizer(text):\n",
        "  return [t.text for t in de_nlp.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxu9o0KzNG-L"
      },
      "source": [
        "# **sequencial data的處理步驟**\n",
        "\n",
        "\n",
        "1.   進行tokenize\n",
        "2.   建立各Language的vocabulary(涵蓋special token-<sos>,<eos>,<pad>,<unk>)\n",
        "3.   將一個sequence加入sos,eos special token\n",
        "4.   轉換成index\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eCdtsdQpIjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d38b89-b1bb-48aa-bd39-48b33e129f9e"
      },
      "source": [
        "#build source sents & target field\n",
        "SRC=torchtext.data.Field(init_token='<sos>',eos_token='<eos>',tokenize=de_tokenizer)\n",
        "TRG=torchtext.data.Field(init_token='<sos>',eos_token='<eos>',tokenize=en_tokenizer)\n",
        "#load data\n",
        "train_data,val_data,test_data=torchtext.datasets.Multi30k.splits(('.de','.en'),(SRC,TRG))\n",
        "print(f'Train exmaples num:{len(train_data)}')\n",
        "print(f'Val exmaples num:{len(val_data)}')\n",
        "print(f'test exmaples num:{len(test_data)}')\n",
        "print(f'One example from Train data:{train_data[0].src}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 856kB/s] \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 224kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 216kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train exmaples num:29000\n",
            "Val exmaples num:1014\n",
            "test exmaples num:1000\n",
            "One example from Train data:['Zwei', 'junge', 'weiße', 'Männer', 'sind', 'im', 'Freien', 'in', 'der', 'Nähe', 'vieler', 'Büsche', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddlpId-8sRhx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60aa98f5-0eeb-408f-fc85-bb059b981800"
      },
      "source": [
        "#build vocab\n",
        "SRC.build_vocab(train_data,min_freq=2,vectors='glove.42B.300d')\n",
        "TRG.build_vocab(train_data,min_freq=2,vectors='glove.42B.300d')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.42B.300d.zip: 1.88GB [14:31, 2.15MB/s]                           \n",
            "100%|█████████▉| 1916654/1917494 [03:50<00:00, 9086.25it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYCsVdfYui7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca5a0ad9-aa67-4977-e267-1fef908bd7f2"
      },
      "source": [
        "print(f'SRC vocab size:{len(SRC.vocab)}')\n",
        "print(f'TRG vocab size:{len(TRG.vocab)}')\n",
        "#set vocab size embedding weight for glove\n",
        "SRC.vocab.set_vectors(SRC.vocab.stoi,SRC.vocab.vectors,dim=300)\n",
        "TRG.vocab.set_vectors(TRG.vocab.stoi,TRG.vocab.vectors,dim=300)\n",
        "print(f'SRC glove embedding size:{SRC.vocab.vectors.size()}')\n",
        "print(f'TRG glove embedding size:{TRG.vocab.vectors.size()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SRC vocab size:8014\n",
            "TRG vocab size:6191\n",
            "SRC glove embedding size:torch.Size([8014, 300])\n",
            "TRG glove embedding size:torch.Size([6191, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8Q3IN9Vwzsj"
      },
      "source": [
        "#set data iterator\n",
        "Batch_size=256\n",
        "train_iter,val_iter,test_iter=torchtext.data.BucketIterator.splits((train_data,val_data,test_data), batch_size=Batch_size,device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69uYy8W5LBUt"
      },
      "source": [
        "# **Encoder(Bidirectional)架構**\n",
        "**模型架構**\n",
        "*   輸入-每個時間點的token id\n",
        "*   輸出-每個時間點的h_state,最後一個時間點的不同layer之h_state\n",
        "\n",
        "**演算流程:**\n",
        "\n",
        "對於每個time step的embedding vector，rnn layer會計算出當前word的h_state，然後if 雙向RNN，則是Output foward,backward的concat hid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOztENlHptRn"
      },
      "source": [
        "#build Encoder Model\n",
        "class BiEncoder(nn.Module):\n",
        "  def __init__(self,input_dim,hid_dim,n_layers,dropout_rate,pretrain_embed=None):\n",
        "    super(BiEncoder,self).__init__()\n",
        "    self.input_dim=input_dim\n",
        "    self.hid_dim=hid_dim\n",
        "    self.n_layers=n_layers\n",
        "\n",
        "    #determined use embed layer whether is pretrained weight or not\n",
        "    if pretrain_embed is None:\n",
        "      self.embed=nn.Embedding(input_dim,hid_dim)\n",
        "    else:\n",
        "      self.embed=nn.Embedding.from_pretrained(pretrain_embed)\n",
        "    self.rnn_layer=nn.GRU(hid_dim,hid_dim,n_layers,dropout=dropout_rate,bidirectional=True)\n",
        "    self.linear_layer=nn.Linear(hid_dim*2,hid_dim)\n",
        "    self.tanh=nn.Tanh()\n",
        "    self.dropout=nn.Dropout(dropout_rate)\n",
        "  def forward(self,input_tensors):\n",
        "    #input tensor shape=[seqL,bs]\n",
        "    embed_input=self.dropout(self.embed(input_tensors))\n",
        "    \n",
        "    outputs,h_state=self.rnn_layer(embed_input)\n",
        "    #output shape=[seq_len,bs,hid_dim*2]\n",
        "    #h_state shape=[layer_num,direction,bs,hid_dim]\n",
        "    #concat forward & backward last layer\n",
        "    h_state=self.tanh(self.linear_layer(torch.cat((h_state[-2,:,:],h_state[-1,:,:]),dim=1)))\n",
        "    return outputs,h_state.unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT-bMY_hqxmC"
      },
      "source": [
        "# **Attention match layer**\n",
        "**模型架構**\n",
        "*   輸入-decoder前一次的h_state,encoder的每個time step hidden output\n",
        "*   輸出-每個time step的attention weight\n",
        "\n",
        "**演算流程:**\n",
        "\n",
        "對於每一個time step，將前一次的decoder h_state進行concat&tranform(tanh)\n",
        "然後再與一個vector(trainable)進行dot-product來得到基於前一次h_state的當前  time step attention weight,最後將每個時間點的attention weight通過softmax來輸出\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKHtXRNEpF2D"
      },
      "source": [
        "#build attention Model\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self,hid_dim):\n",
        "    super(Attention,self).__init__()\n",
        "    self.attn_layer=nn.Linear(2*hid_dim+hid_dim,hid_dim)\n",
        "    self.v=nn.Linear(hid_dim,1,bias=False)\n",
        "    self.softmax=nn.Softmax(dim=1)\n",
        "    self.tanh=nn.Tanh()\n",
        "  def forward(self,decoder_hidden,encoder_outputs):\n",
        "    batch_size=encoder_outputs.size(1)\n",
        "    seqLen=encoder_outputs.size(0)\n",
        "\n",
        "    decoder_hidden=decoder_hidden.unsqueeze(1)#insert dim shape=[bs,1,hid_dim]\n",
        "    decoder_hidden=decoder_hidden.repeat(1,seqLen,1)\n",
        "\n",
        "    #concat decoder_hidden & encoder_outputs\n",
        "    #shape=[bs,seqLen,encoder_hid+decoder_hid]\n",
        "    attn_hid=torch.cat((decoder_hidden,encoder_outputs.permute(1,0,2)),dim=2)\n",
        "    #non-linear transform\n",
        "    attn_hid=self.tanh(self.attn_layer(attn_hid))\n",
        "    #compute attnetion weight\n",
        "    #shape=[bs,seqLen]\n",
        "    attn_weight=self.softmax(self.v(attn_hid).squeeze(2))\n",
        "\n",
        "    return attn_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7eqMDZb-qM3"
      },
      "source": [
        "# **建立Decoder**\n",
        "\n",
        "**Decoder架構**\n",
        "*   輸入-前一個時間點預測的word Embedding,h_state以及關注encoder ouutput的attention weight,encoder context word\n",
        "*   輸出-當前時間點預測的word Dist. 以及hidden_state\n",
        "\n",
        " \n",
        " **演算過程**:\n",
        "  將輸入word的Embedding與Context(將encoder output藉由attention weight來weight sum)進行concat然後輸入至rnn_layer中，隨後將rnn layer的hidden_state與input word embedding,context vector送進linear classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY-LCImVyr5J"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,input_dim,hid_dim,output_dim,n_layers,dropout_rate,pretrain_embed=None):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.input_dim=input_dim\n",
        "    self.hid_dim=hid_dim\n",
        "    self.output_dim=output_dim\n",
        "    self.n_layers=n_layers\n",
        "\n",
        "    #determined use embed layer whether is pretrained weight or not\n",
        "    if pretrain_embed is None:\n",
        "      self.embed=nn.Embedding(input_dim,hid_dim)\n",
        "    else:\n",
        "      self.embed=nn.Embedding.from_pretrained(pretrain_embed)\n",
        "    self.rnn_layer=nn.GRU(hid_dim*3,hid_dim,n_layers,dropout=dropout_rate)\n",
        "    self.fc=nn.Linear(hid_dim+hid_dim+hid_dim*2,output_dim)\n",
        "  def forward(self,input_tensors,hidden_state,attn_weight,encoder_outputs):\n",
        "    input_embed=self.embed(input_tensors)\n",
        "    context_vector=torch.bmm(attn_weight.unsqueeze(dim=1),encoder_outputs)[:,0,:]\n",
        "    #compute input rnn tensor and insert dim0\n",
        "    input_tensor=torch.cat((input_embed,context_vector),dim=1).unsqueeze(0)\n",
        "    \n",
        "    outputs,h_state=self.rnn_layer(input_tensor,hidden_state)\n",
        "    #output shape=[bs,output_dim]\n",
        "    outputs=self.fc(torch.cat((input_embed,context_vector,outputs[0]),dim=1))\n",
        "    return outputs,h_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAgbLTeXJBfo"
      },
      "source": [
        "# **Sequence to Sequence Model(Coditional Generation)**\n",
        "\n",
        "模型架構\n",
        "\n",
        "\n",
        "*   輸入-source sentences index.\n",
        "*   輸出-target sentences index.\n",
        "\n",
        "演算流程\n",
        " \n",
        "\n",
        "1.   將src sents 透過encoder進行encoding,得到該sent每個word的Repr. 以及 最後一個time step的hidden state\n",
        "2.   將context vector視為最初要輸入至decoder的hidden state，sos token id則設置維第一個要輸入至decoder的word\n",
        "3.   以下流程則是反覆iter seqlen+1次\n",
        "    \n",
        "\n",
        "*   使用attention layer計算前一次h_state與encoder output的attention\n",
        "*   將得到的attention weight與encoder output進行weighted sum得到context vector\n",
        "*   將context vector、前一次decoder 預測的詞彙之word embedding & h_sate輸入至decoder中\n",
        "*   將當前decoder輸出的h_state 與前一次的word embedding以及context vector輸入至linear classifier\n",
        "*   將Linear classifier的output以及當前計算的h_state當作是下一個decoder要輸入的word,h_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ0NFtauJApG"
      },
      "source": [
        "class AttnSeq2Seq(nn.Module):\n",
        "  def __init__(self,encoder,decoder,attner,device):\n",
        "    super(AttnSeq2Seq,self).__init__()\n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.attner=attner\n",
        "    self.device=device\n",
        "    \n",
        "    assert self.encoder.hid_dim==self.decoder.hid_dim\n",
        "  def forward(self,src_tensors,trg_tensors,teach_ratio):\n",
        "    seqLen=trg_tensors.size(0)\n",
        "    Batch_size=trg_tensors.size(1)\n",
        "    OutputVocab=self.decoder.output_dim\n",
        "    preds=torch.zeros(seqLen,Batch_size,OutputVocab,device=self.device)\n",
        "    #get seq hidden vector and Context vector\n",
        "    encoder_outputs,final_hidden=self.encoder(src_tensors)\n",
        "\n",
        "    decoder_hidden=final_hidden\n",
        "    decoder_input=trg_tensors[0,:]\n",
        "    for ti in range(1,seqLen):\n",
        "      #compute attention weight\n",
        "      #shape=[bs,seqLen]\n",
        "      attn_weight=self.attner(decoder_hidden[-1,:,:],encoder_outputs)\n",
        "      #output vector,shape=[bs,vocab]\n",
        "      #decoder hidden,shape=[n_layers,bs,hid_dim]\n",
        "      decoder_outputs,decoder_hidden=self.decoder(decoder_input,decoder_hidden,attn_weight,encoder_outputs.permute(1,0,2))\n",
        "      preds[ti]=decoder_outputs\n",
        "      \n",
        "      #determined next input using teacher forcing or not\n",
        "      teach_force=True if random.random()<teach_ratio else False\n",
        "      top1=decoder_outputs.argmax(dim=1)\n",
        "      decoder_input=trg_tensors[ti] if teach_force else top1\n",
        "    \n",
        "    #preds shape=[seq_len,bs,vocab_dim]\n",
        "    return preds\n",
        "  def getPredict(self,src_tensors,trg_initTokenId,end_tokenId):\n",
        "    preds=[]\n",
        "\n",
        "    #get hidden_vector,context vector\n",
        "    encoder_outputs,final_hidden=self.encoder(src_tensors)\n",
        "    decoder_hidden=final_hidden\n",
        "    decoder_input=torch.tensor([trg_initTokenId],device=self.device)\n",
        "    \n",
        "    while True:\n",
        "      #compute attention weight\n",
        "      attn_weight=self.attner(decoder_hidden[-1,:,:],encoder_outputs)\n",
        "      \n",
        "      #output\n",
        "      decoder_outputs,decoder_hidden=self.decoder(decoder_input,decoder_hidden,attn_weight,encoder_outputs.permute(1,0,2))\n",
        "\n",
        "      top1=decoder_outputs.argmax(dim=1)\n",
        "      decoder_input=top1\n",
        "      #append to preds\n",
        "      preds.append(decoder_input.item())\n",
        "      if decoder_input.item()==end_tokenId:\n",
        "        break\n",
        "    #return list of token id\n",
        "    return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqsoXT51vdzU"
      },
      "source": [
        "#set weight except bias value for N(0,1) value and bias weight for 0\n",
        "def init_weights(m):\n",
        "  for named,params in m.named_parameters():\n",
        "    if 'weight' in named:\n",
        "      torch.nn.init.normal_(params.data,0,0.01)\n",
        "    else:\n",
        "      torch.nn.init.constant_(params.data,0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO2tAexHc0ri"
      },
      "source": [
        "Define training Model process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2US1ARWMc0O8"
      },
      "source": [
        "encoder_config={'input_dim':len(SRC.vocab),'hid_dim':256,\n",
        "        'n_layers':2,'dropout_rate':0.3,\n",
        "        'pretrain_embed':None}\n",
        "decoder_config={'input_dim':len(TRG.vocab),'hid_dim':256,\n",
        "        'output_dim':len(TRG.vocab),'n_layers':1,\n",
        "        'dropout_rate':0,'pretrain_embed':None}\n",
        "attner_config={'hid_dim':256}\n",
        "model_configs={\n",
        "    'encoder':encoder_config,\n",
        "    'decoder':decoder_config,\n",
        "    'attner':attner_config,\n",
        "}\n",
        "EPOCHS=20\n",
        "GRAD_NORM=1\n",
        "Learning_rate=1e-3\n",
        "model_dir='./MTmodel'\n",
        "model_path='fra2eng_model.pt'\n",
        "config_path='fra2eng_training_config.bin'\n",
        "per_epoch_evaluate=5\n",
        "#build Model\n",
        "encoder_model=BiEncoder(**encoder_config)\n",
        "decoder_model=Decoder(**decoder_config)\n",
        "attn_model=Attention(**attner_config)\n",
        "model=AttnSeq2Seq(encoder_model,decoder_model,attn_model,device)\n",
        "model.to(device)\n",
        "model.apply(init_weights)\n",
        "#build optimizer &loss func\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=Learning_rate)\n",
        "criterion=nn.CrossEntropyLoss(ignore_index=TRG.vocab.stoi['<pad>'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOvlBxSzgLxg"
      },
      "source": [
        "#save Model and model config\n",
        "def save_model(model_dir,model_path,config_path,model,optimizer,configs,epochs):\n",
        "  if not os.path.isdir(model_dir):\n",
        "    print('Model dir is not existed')\n",
        "    os.mkdir(model_dir)\n",
        "  else:\n",
        "    print('Model dir already existed')\n",
        "  #save model state and config\n",
        "  print(f'Save Model to dir:{model_dir}')\n",
        "  model_state={'model':model.state_dict(),'optimizer':optimizer.state_dict(),'epochs':epochs}\n",
        "\n",
        "  torch.save(model_state,os.path.join(model_dir,model_path))\n",
        "  torch.save(configs,os.path.join(model_dir,config_path))\n",
        "  print('Save Model success!')\n",
        "\n",
        "#load Model\n",
        "def load_model(model_dir,model_path,device):\n",
        "  if not os.path.isdir(model_dir):\n",
        "    raise Exception('Model dir is not existed')\n",
        "  else:\n",
        "    load_path=os.path.join(model_dir,model_path)\n",
        "    try:\n",
        "      state_dict=torch.load(load_path,map_location=device)\n",
        "      print('Read model file is successed')\n",
        "      return state_dict\n",
        "    except:\n",
        "      raise Exception('Model file is loss...')\n",
        "  \n",
        "#training Model stage\n",
        "def training(model,train_iters,optimizer,criterion,teach_ratio):\n",
        "  progress_bar=tqdm.tqdm(train_iters,desc='Iteration')\n",
        "  epoch_loss=0\n",
        "  for b in progress_bar:\n",
        "    #fetch tensors\n",
        "    src_tensors=b.src\n",
        "    trg_tensors=b.trg\n",
        "    preds=model(src_tensors,trg_tensors,teach_ratio)\n",
        "    #compute loss\n",
        "    loss=criterion(preds[1:,:,:].view(-1,preds.size(2)),trg_tensors[1:,:].view(-1))\n",
        "    loss.backward()\n",
        "\n",
        "    #clipping gradient\n",
        "    nn.utils.clip_grad_norm_(model.parameters(),max_norm=1)\n",
        "    #update params\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    epoch_loss+=loss.item()\n",
        "  progress_bar.close()\n",
        "  return epoch_loss/len(train_iters)\n",
        "def get_evaluate(model,eval_data,de_field,en_field,device):\n",
        "  #random sampling example from eval_data\n",
        "  idx=next(iter(RandomSampler(eval_data)))\n",
        "  src_sents=['<sos>']+eval_data[idx].src+['<eos>']\n",
        "  trg_sents=eval_data[idx].trg\n",
        "  #convert tensors shape=[seqLen,batch]\n",
        "  src_tensors=de_field.numericalize([src_sents],device=device)\n",
        "  print(f'Origin eval sents:{src_sents}')\n",
        "  print(f'Origin eval sents index:{src_tensors}')\n",
        "\n",
        "  #translate sent\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    pred_index=model.getPredict(src_tensors,en_field.vocab.stoi['<sos>'],en_field.vocab.stoi['<eos>'])\n",
        "  pred_sent=[en_field.vocab.itos[id] for id in pred_index]\n",
        "\n",
        "  print(f'Original target sents:{trg_sents}')\n",
        "  print(f'Translated target sents:{pred_sent}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQfa_A5SMthK"
      },
      "source": [
        "可修該的不同方法\n",
        "1.init Embedding,GLoVe,fastText\n",
        "2.Gated Rucurrent Unit\n",
        " Layer-2,3\n",
        " hid_dim-256\n",
        "3.輸出到Decoder的context vector 的策略\n",
        "  3.1.將第一層,最後一層layer的forward,backward h_state進行concat,然後non-linear transform (tanh)\n",
        "  3.2採用forward or backward的每層layer h_state進行concat transform or pooling(Max,average)\n",
        "  3.3只採用backward or forward 的第一、最後一層layer h_state"
      ]
    }
  ]
}