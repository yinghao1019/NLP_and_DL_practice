{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Packed_PAD(Prac).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yinghao1019/NLP_and_DL_practice/blob/master/Packed_PAD(Prac).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeqwOVz3j2-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3042d8ce-d48f-4554-c7a1-d5588e1d92e1"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch import nn,optim\r\n",
        "import torchtext\r\n",
        "from torchtext import datasets\r\n",
        "from torchtext.data import Field,BucketIterator\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import spacy\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import random\r\n",
        "import tqdm\r\n",
        "import math\r\n",
        "import os\r\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI3mK2dUn1n8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d6db811-4b71-4ea7-9719-59fc2c8e2244"
      },
      "source": [
        "#build each tokenizer\r\n",
        "en_nlp=spacy.load('en')\r\n",
        "de_nlp=spacy.load('de')\r\n",
        "def tokenize_en(text):\r\n",
        "  return [t.text for t in en_nlp.tokenizer(text)]\r\n",
        "def tokenize_de(text):\r\n",
        "  return [t.text for t in de_nlp.tokenizer(text)]\r\n",
        "#build source and target sent field \r\n",
        "SRC=Field(init_token='<sos>',eos_token='<eos>',tokenize=tokenize_de,include_lengths=True)\r\n",
        "TRG=Field(init_token='<sos>',eos_token='<eos>',tokenize=tokenize_en)\r\n",
        "train_data,val_data,test_data=datasets.Multi30k.splits(exts=('.de','.en'),fields=(SRC,TRG))\r\n",
        "#build vocabulary\r\n",
        "SRC.build_vocab(train_data,min_freq=2)\r\n",
        "TRG.build_vocab(train_data,min_freq=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 1.04MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 273kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 266kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVpGXk55xesC"
      },
      "source": [
        "BATCH_SIZE=64\r\n",
        "device=torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "train_iter=BucketIterator(train_data,device=device,\r\n",
        "                          batch_size=BATCH_SIZE,sort_key=lambda x:len(x.src),\r\n",
        "                          sort_within_batch=True,shuffle=True)\r\n",
        "val_iter,test_iter=BucketIterator.splits((val_data,test_data),\r\n",
        "                                          device=device,\r\n",
        "                                          batch_size=BATCH_SIZE,\r\n",
        "                                          sort_key=lambda x:len(x.src),\r\n",
        "                                          sort_within_batch=True)\r\n",
        "\r\n",
        "class Encoder(nn.Module):\r\n",
        "  def __init__(self,input_dim,embed_dim,encoder_hid_dim,decoder_hid_dim,n_layers,drop_rate):\r\n",
        "    super(Encoder,self).__init__()\r\n",
        "    self.decode_hid_dim=decoder_hid_dim\r\n",
        "    self.input_dim=input_dim\r\n",
        "    self.encode_hid_dim=encoder_hid_dim\r\n",
        "    self.n_layers=n_layers\r\n",
        "\r\n",
        "    self.embed=nn.Embedding(input_dim,embed_dim)\r\n",
        "    self.rnn_layer=nn.GRU(embed_dim,encoder_hid_dim,n_layers,bidirectional=True)\r\n",
        "    self.fc_layer=nn.Linear(encoder_hid_dim*2,decoder_hid_dim)\r\n",
        "\r\n",
        "    self.dropout_layer=nn.Dropout(drop_rate)\r\n",
        "    self.tanh=nn.Tanh()\r\n",
        "  def forward(self,input_tensors,input_lens):\r\n",
        "    #input_tensor=[seqlen,bs]\r\n",
        "    #input_lens=[]\r\n",
        "    #input_embed=[seqlen,bs,embed_dim]\r\n",
        "\r\n",
        "    #embedding input_tensor\r\n",
        "    input_embed=self.embed(input_tensors)\r\n",
        "    #packed embed tensor\r\n",
        "    packed_embed=torch.nn.utils.rnn.pack_padded_sequence(input_embed,input_lens.cpu()).cuda()\r\n",
        "    packed_output,hidden=self.rnn_layer(packed_embed)\r\n",
        "    #unpack packed output to padded output\r\n",
        "    padded_seq,_=torch.nn.utils.rnn.pad_packed_sequence(packed_output)\r\n",
        "    #transform encoder final layer forward &backward into init decoder hidden state\r\n",
        "    hidden=self.tanh(self.fc_layer(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1)))\r\n",
        "\r\n",
        "    return padded_seq,hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K996WoxTJhF6"
      },
      "source": [
        "class Attention(nn.Module):\r\n",
        "  def __init__(self,encoder_hid_dim,decoder_hid_dim):\r\n",
        "    super(Attention,self).__init__()\r\n",
        "    self.fc_layer=nn.Linear(encoder_hid_dim*2+decoder_hid_dim,decoder_hid_dim)\r\n",
        "    self.attn=nn.Linear(decoder_hid_dim,1,bias=False)\r\n",
        "\r\n",
        "    self.softmax=nn.Softmax(dim=1)\r\n",
        "    self.tanh=nn.Tanh()\r\n",
        "  def forward(self,hidden,encoder_outputs,mask):\r\n",
        "    #hidden=[bs,1,hidden_dim]\r\n",
        "    #encoder_outputs=[bs,seqlen,encoder_hidden_dim*2]\r\n",
        "    #mask=[bs,seqlen]\r\n",
        "\r\n",
        "    seqlen=encoder_outputs.shape[1]\r\n",
        "    hidden=hidden.unsqueeze(1)\r\n",
        "    #copy hidden data\r\n",
        "    hidden=hidden.repeat(1,seqlen,1)\r\n",
        "    #concat encoder output & transform into hidden dim\r\n",
        "    context_vector=self.tanh(self.fc_layer(torch.cat((encoder_outputs,hidden),dim=2)))\r\n",
        "\r\n",
        "    #transform to attn weight alpha(before softmax)\r\n",
        "    #attn_w=[bs,seqlen,1]\r\n",
        "    attn_w=self.attn(context_vector).squeeze(2)\r\n",
        "    #fill 1e-10 along mask\r\n",
        "    attn_w=attn_w.masked_fill_(mask==0,-1e10)\r\n",
        "\r\n",
        "    return self.softmax(attn_w).unsqueeze(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUMGk8z958le"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "  def __init__(self,embed_dim,encoder_hid_dim,decoder_hid_dim,output_dim,attention,dropout_rate):\r\n",
        "    super(Decoder,self).__init__()\r\n",
        "    self.output_dim=output_dim\r\n",
        "    self.encoder_hid_dim=encoder_hid_dim\r\n",
        "    self.decoder_hid_dim=decoder_hid_dim\r\n",
        "    self.embed_dim=embed_dim\r\n",
        "    self.attn=attention\r\n",
        "\r\n",
        "    #build layer\r\n",
        "    self.embed=nn.Embedding(output_dim,embed_dim)\r\n",
        "    self.rnn_layer=nn.GRU((encoder_hid_dim*2+embed_dim),decoder_hid_dim)\r\n",
        "    self.fc_layer=nn.Linear((encoder_hid_dim*2+decoder_hid_dim+embed_dim),output_dim)\r\n",
        "\r\n",
        "    self.dropout=nn.Dropout(dropout_rate)\r\n",
        "  def forward(self,input_word,hidden_state,encoder_outputs,mask):\r\n",
        "\r\n",
        "    #shape\r\n",
        "    #input_word=[1,bs]\r\n",
        "    #hidden_state=[1,bs,decoder_hid_dim]\r\n",
        "    #mask=[bs,seqlen]\r\n",
        "    #encoder_output=[bs,seqlen,encoder_hid]\r\n",
        "\r\n",
        "    #input_embed=[1,bs,embed_dim]\r\n",
        "    input_embed=self.embed(input_word).unsqueeze(0)\r\n",
        "\r\n",
        "    #compute current word attention vector\r\n",
        "    #attn_w=[bs,seq,1]\r\n",
        "    attn_w=self.attn(hidden_state,encoder_outputs,mask)\r\n",
        "    #context vector=[bs,1,encoder_hid]\r\n",
        "    context_vector=torch.bmm(attn_w.permute(0,2,1),encoder_outputs)\r\n",
        "    context_vector=context_vector.permute(1,0,2)\r\n",
        "\r\n",
        "\r\n",
        "    decoder_output,hidden_state=self.rnn_layer(torch.cat((context_vector,input_embed),dim=2),hidden_state.unsqueeze(0))\r\n",
        "\r\n",
        "    assert (decoder_output==hidden_state).all()\r\n",
        "\r\n",
        "    context_vector=context_vector.squeeze(0)\r\n",
        "    input_embed=input_embed.squeeze(0)\r\n",
        "    hidden_state=hidden_state.squeeze(0)\r\n",
        "\r\n",
        "    #pred logitics=[bs,output_dim]\r\n",
        "    pred_logitics=self.fc_layer(torch.cat((input_embed,hidden_state,context_vector),dim=1))\r\n",
        "\r\n",
        "    return pred_logitics,hidden_state,attn_w.squeeze(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BGncX5cOe6f"
      },
      "source": [
        "class seq2seq(nn.Module):\r\n",
        "  def __init__(self,input_dim,embed_dim,encoder_hid_dim,\r\n",
        "               decoder_hid_dim,output_dim,n_layers,\r\n",
        "               src_pad_idx,dropout_rate,device):\r\n",
        "    super(seq2seq,self).__init__()\r\n",
        "\r\n",
        "    #build each sub module(encoder,decoder,attention)\r\n",
        "    self.encoder=Encoder(input_dim,embed_dim,\r\n",
        "                         encoder_hid_dim,decoder_hid_dim,\r\n",
        "                         n_layers,dropout_rate)\r\n",
        "    self.attner=Attention(encoder_hid_dim,decoder_hid_dim)\r\n",
        "    self.decoder=Decoder(embed_dim,encoder_hid_dim,\r\n",
        "                         decoder_hid_dim,output_dim,\r\n",
        "                         self.attner,dropout_rate)\r\n",
        "    \r\n",
        "    self.src_pad_idx=src_pad_idx\r\n",
        "    self.softmax=nn.Softmax(dim=1)\r\n",
        "    self.device=device\r\n",
        "  def create_mask(self,src_tensor):\r\n",
        "    #mask=[bs,seqlen]\r\n",
        "    mask=(src_tensor!=self.src_pad_idx).permute(1,0)\r\n",
        "    return mask\r\n",
        "  def forward(self,src_tensor,src_len,trg_tensor,teaching_forcing_ratio=0.5):\r\n",
        "    #src_tensor=[seqlen,bs]\r\n",
        "    #src_len=[bs]\r\n",
        "    #trg_tensor=[seqlen,bs]\r\n",
        "\r\n",
        "    batch_size=trg_tensor.shape[1]\r\n",
        "    seqlen=trg_tensor.shape[0]\r\n",
        "    src_seqlen=src_tensor.shape[0]\r\n",
        "    trg_vocab=self.decoder.output_dim\r\n",
        "\r\n",
        "    #build storage decoder predicts\r\n",
        "    trg_preds=torch.zeros(seqlen,batch_size,trg_vocab,device=self.device)\r\n",
        "    #build storage decoder each token attn weight\r\n",
        "    attentions=torch.zeros(seqlen,batch_size,src_seqlen,device=self.device)\r\n",
        "\r\n",
        "    #encoder stage\r\n",
        "    encoder_outputs,hidden=self.encoder(src_tensor,src_len)\r\n",
        "    encoder_mask=self.create_mask(src_tensor)\r\n",
        "\r\n",
        "    encoder_outputs=encoder_outputs.permute(1,0,2)\r\n",
        "    decoder_output=trg_tensor[0,:]\r\n",
        "    decoder_hidden=hidden\r\n",
        "\r\n",
        "    #decoder stage\r\n",
        "    for ti in range(1,seqlen):\r\n",
        "      decoder_output,decoder_hidden,decoder_attn=self.decoder(decoder_output,decoder_hidden,encoder_outputs,encoder_mask)\r\n",
        "\r\n",
        "      #storage attn_w & preds\r\n",
        "      trg_preds[ti]=decoder_output\r\n",
        "      attentions[ti]=decoder_attn\r\n",
        "\r\n",
        "      if random.random()<teaching_forcing_ratio:\r\n",
        "        decoder_output=trg_tensor[ti,:]\r\n",
        "      else:\r\n",
        "        decoder_output=self.softmax(decoder_output)\r\n",
        "        #argmax proba word index\r\n",
        "        decoder_output=torch.argmax(decoder_output,dim=1)\r\n",
        "    #trg_preds=[seqlen,bs,output_dim]\r\n",
        "    #attentions=[seqlen,bs,seqlen]\r\n",
        "    return trg_preds,attentions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj3PrygwlcRb"
      },
      "source": [
        "def getBatchCorrect(predict_tensor,true_tensor,true_len):\r\n",
        "  correct=0\r\n",
        "  assert predict_tensor.shape[0]==true_tensor.shape[0]==len(true_len)\r\n",
        "  batch_size=predict_tensor.shape[0]\r\n",
        "\r\n",
        "  for i in range(batch_size):\r\n",
        "    correct+=torch.equal(predict_tensor[i,1:true_len[i]],true_tensor[i,1:true_len[i]])\r\n",
        "  \r\n",
        "  return correct\r\n",
        "def saveModel(model_dir,model_path,model,optim,model_info,ep):\r\n",
        "  if os.path.isdir(model_dir):\r\n",
        "    print('Model dir already exists')\r\n",
        "  else:\r\n",
        "    print('Model dir not exists')\r\n",
        "    os.mkdir(model_dir)\r\n",
        "    print('Model dir already build!')\r\n",
        "\r\n",
        "  save_path=os.path.join(model_dir,model_path)\r\n",
        "  torch.save({\r\n",
        "      'model':model.state_dict(),\r\n",
        "      'optimizer':optim.state_dict(),\r\n",
        "      'model_info':model_info,\r\n",
        "      'EPOCHS':ep,\r\n",
        "  },save_path)\r\n",
        "  print(f'Already save model to {save_path}')\r\n",
        "def init_weight(m):\r\n",
        "  for name,parameters in m.named_parameters():\r\n",
        "    if 'weight' in name:\r\n",
        "      torch.nn.init.normal_(parameters.data,mean=0,std=0.01)\r\n",
        "    else:\r\n",
        "      torch.nn.init.constant_(parameters.data,0)\r\n",
        "def count_parameters(model):\r\n",
        "  return sum(param.numel() for param in model.parameters() if param.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhJAVeK5UJ-i"
      },
      "source": [
        "def train(model,train_loader,optimizer,loss_fn,clip):\r\n",
        "  epoch_loss=0\r\n",
        "  for b_data in train_loader:\r\n",
        "      src_tensors,src_lens=b_data.src\r\n",
        "      trg_tensors=b_data.trg\r\n",
        "      #output=[seqlen,bs,vocab_size]\r\n",
        "      model_output,_=model(src_tensors,src_lens,trg_tensors)\r\n",
        "      model_output=model_output[1:].view(-1,model.decoder.output_dim)\r\n",
        "      trg_tensors=trg_tensors[1:].view(-1)\r\n",
        "      #compute loss\r\n",
        "      loss=loss_fn(model_output,trg_tensors)\r\n",
        "      loss.backward()\r\n",
        "      epoch_loss+=loss.item()\r\n",
        "      \r\n",
        "      #gradient scaling\r\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\r\n",
        "      \r\n",
        "      optimizer.step()\r\n",
        "      optimizer.zero_grad()\r\n",
        "\r\n",
        "  return epoch_loss/len(train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLElEoE_QlfI"
      },
      "source": [
        "def evaluateModel(model,val_loader,loss_fn):\r\n",
        "  correct_num=0\r\n",
        "  total_loss=0\r\n",
        "  model.eval()\r\n",
        "  with torch.no_grad():\r\n",
        "    for batch in val_loader:\r\n",
        "      src_tensors,src_len=batch.src\r\n",
        "      trg_tensors=batch.trg\r\n",
        "\r\n",
        "      model_output,_=model(src_tensors,src_len,trg_tensors,teaching_forcing_ratio=0)\r\n",
        "      \r\n",
        "      #compute batch loss\r\n",
        "      loss=loss_fn(model_output[1:].view(-1,model.decoder.output_dim),trg_tensors[1:].view(-1))\r\n",
        "      total_loss+=loss\r\n",
        "\r\n",
        "      #predict_tensors=[seqlen,bs]\r\n",
        "      predict_tensors=torch.argmax(F.softmax(model_output[1:],dim=2),dim=2)\r\n",
        "  \r\n",
        "  return total_loss/len(val_loader)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gjmXo47ryv7"
      },
      "source": [
        "def train_pipeline(model,train_loader,val_loader,optimizer,loss_fn,epochs,clip,model_dir,model_path,model_structer):\r\n",
        "  for ep in range(epochs):\r\n",
        "    model.train()\r\n",
        "    ep_trainLoss=train(model,train_loader,optimizer,loss_fn,clip)\r\n",
        "    ep_valLoss=evaluateModel(model,val_loader,loss_fn)\r\n",
        "    \r\n",
        "    print('[{}/{}] train loss:{} val loss:{}'.format(ep,epochs,ep_trainLoss,ep_valLoss))\r\n",
        "    saveModel(model_dir,model_path,model,optimizer,model_structer,ep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvBjbd_kVjJy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4abd0eb7-55ad-43bc-aa11-25c86694dd42"
      },
      "source": [
        "BATCH_size=128\r\n",
        "input_dim=len(SRC.vocab)\r\n",
        "output_dim=len(TRG.vocab)\r\n",
        "embed_dim=256\r\n",
        "encoder_hid_dim=512\r\n",
        "decoder_hid_dim=512\r\n",
        "dropout_rate=0.5\r\n",
        "epochs=10\r\n",
        "n_layers=1\r\n",
        "clip=1\r\n",
        "src_pad_idx=SRC.vocab.stoi[SRC.pad_token]\r\n",
        "trg_pad_idx=TRG.vocab.stoi[TRG.pad_token]\r\n",
        "model_dir='./Model'\r\n",
        "model_path='seq2seq.pt'\r\n",
        "\r\n",
        "#build Model dict\r\n",
        "model_dict={'input_dim':input_dim,'embed_dim':embed_dim,'encoder_hid_dim':encoder_hid_dim,'decoder_hid_dim':decoder_hid_dim,\r\n",
        "            'output_dim':output_dim,'n_layers':n_layers,'src_pad_idx':src_pad_idx,'dropout_rate':dropout_rate,'device':device}\r\n",
        "model=seq2seq(**model_dict)\r\n",
        "#set model init weight\r\n",
        "model.apply(init_weight)\r\n",
        "\r\n",
        "model.to(device)\r\n",
        "#conut model total parameters\r\n",
        "print('Model total parameters num:{}'.format(count_parameters(model)))\r\n",
        "#build optimizer & loss\r\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.001)\r\n",
        "criterion=nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\r\n",
        "train_pipeline(model,train_iter,val_iter,optimizer,criterion,epochs,clip,model_dir,model_path,model_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model total parameters num:21170223\n",
            "[0/10] train loss:4.824056623790757 val loss:4.62297248840332\n",
            "Model dir not exists\n",
            "Model dir already build!\n",
            "Already save model to ./Model/seq2seq.pt\n",
            "[1/10] train loss:3.4048942674099085 val loss:3.648510217666626\n",
            "Model dir already exists\n",
            "Already save model to ./Model/seq2seq.pt\n",
            "[2/10] train loss:2.6091284371157575 val loss:3.2830631732940674\n",
            "Model dir already exists\n",
            "Already save model to ./Model/seq2seq.pt\n",
            "[3/10] train loss:2.1177981443342135 val loss:3.1920197010040283\n",
            "Model dir already exists\n",
            "Already save model to ./Model/seq2seq.pt\n",
            "[4/10] train loss:1.7799209717349334 val loss:3.2499167919158936\n",
            "Model dir already exists\n",
            "Already save model to ./Model/seq2seq.pt\n",
            "[5/10] train loss:1.4866672822557356 val loss:3.337244749069214\n",
            "Model dir already exists\n",
            "Already save model to ./Model/seq2seq.pt\n",
            "[6/10] train loss:1.2822412714559077 val loss:3.4594545364379883\n",
            "Model dir already exists\n",
            "Already save model to ./Model/seq2seq.pt\n",
            "[7/10] train loss:1.099757159990361 val loss:3.53216552734375\n",
            "Model dir already exists\n",
            "Already save model to ./Model/seq2seq.pt\n",
            "[8/10] train loss:0.9389869684952471 val loss:3.608555316925049\n",
            "Model dir already exists\n",
            "Already save model to ./Model/seq2seq.pt\n",
            "[9/10] train loss:0.7987453418943851 val loss:3.8055317401885986\n",
            "Model dir already exists\n",
            "Already save model to ./Model/seq2seq.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOVnuEjJ5j1b"
      },
      "source": [
        "def ModelTranslation(model,translated_sents,SRC_Field,TRG_Field,device):\r\n",
        "  attn_list=None\r\n",
        "  token_indexes=[TRG_Field.vocab.stoi[TRG_Field.init_token]]\r\n",
        "\r\n",
        "  #source sentences preprocess\r\n",
        "  if isinstance(translated_sents,str):\r\n",
        "    src_tokens=tokenize_de(translated_sents)\r\n",
        "  else:\r\n",
        "    src_tokens=[t.lower() for t in translated_sents]\r\n",
        "\r\n",
        "  src_tokens=[SRC_Field.init_token]+src_tokens+[SRC_Field.eos_token]\r\n",
        "  src_seqlen=len(src_tokens)\r\n",
        "\r\n",
        "  src_tensor,srcLen_tensor=SRC_Field.numericalize(([src_tokens],[src_seqlen]),device=device)\r\n",
        "\r\n",
        "  #get encoder output & hidden state from encoder\r\n",
        "  with torch.no_grad():\r\n",
        "    encoder_outputs,encoder_hidden=model.encoder(src_tensor,srcLen_tensor)\r\n",
        "    src_masking=model.create_mask(src_tensor)#get masking\r\n",
        "\r\n",
        "    encoder_outputs=encoder_outputs.permute(1,0,2)\r\n",
        "    decoder_hidden=encoder_hidden\r\n",
        "\r\n",
        "    while True:\r\n",
        "      decoder_output=torch.tensor([token_indexes[-1]],dtype=torch.long,device=device)\r\n",
        "\r\n",
        "      #decoder_output=[bs,output_dim]\r\n",
        "      #decoder_attn=[bs,src_seqlen]\r\n",
        "      #decoder_hidden=[layers,bs,decoder_hid]\r\n",
        "      decoder_output,decoder_hidden,decoder_attn=model.decoder(decoder_output,decoder_hidden,encoder_outputs,src_masking)\r\n",
        "\r\n",
        "      if attn_list is None:\r\n",
        "        attn_list=decoder_attn\r\n",
        "      else:\r\n",
        "        attn_list=torch.cat((attn_list,decoder_attn),0)\r\n",
        "      \r\n",
        "      #get decoder next time input\r\n",
        "      decoder_output=torch.argmax(F.softmax(decoder_output,dim=1),dim=1)\r\n",
        "      token_indexes.append(decoder_output.item())\r\n",
        "\r\n",
        "      if decoder_output.item()==TRG_Field.vocab.stoi[TRG_Field.eos_token]:\r\n",
        "        break\r\n",
        "\r\n",
        "    predict_tokens=[TRG.vocab.itos[t_id] for t_id in token_indexes]\r\n",
        "    return predict_tokens[1:],attn_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDfpdoSvm2Vu"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "#define plot translation lang attn weight\r\n",
        "def display_attnetion(sentences,translated,attention):\r\n",
        "  fig=plt.figure(figsize=(10,10))\r\n",
        "  ax=fig.add_subplot(1,1,1)\r\n",
        "\r\n",
        "  ax.matshow(attention,cmap='bone')\r\n",
        "  ax.tick_params(labelsize=15)\r\n",
        "  ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentences]+['<eos>'],rotation=0.45)\r\n",
        "  ax.set_yticklabels(['']+translated)\r\n",
        "\r\n",
        "  #set tick locate\r\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "\r\n",
        "  plt.show()\r\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z8iwOq91bAi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "538296c1-4234-4b2f-9d03-f1781a6dc9f6"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\r\n",
        "def compute_bleu(model,data,SRC_Field,TRG_Field,device):\r\n",
        "  trans_corpus=[]\r\n",
        "  ref_corpus=[]\r\n",
        "\r\n",
        "  for data in data:\r\n",
        "    src_tensors=vars(data)['src']\r\n",
        "    trg_tensors=vars(data)['trg']\r\n",
        "\r\n",
        "    predict_tensor,_=ModelTranslation(model,src_tensors,SRC_Field,TRG_Field,device)\r\n",
        "\r\n",
        "    trans_corpus.append(predict_tensor[:-1])\r\n",
        "    ref_corpus.append([trg_tensors])\r\n",
        "\r\n",
        "  return bleu_score(trans_corpus,ref_corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7b879d0f67f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSRC_Field\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTRG_Field\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mtrans_corpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mref_corpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.data.metrics'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY-4XHPv14_0"
      },
      "source": [
        "#test language\r\n",
        "examples_id=3\r\n",
        "src_example=vars(test_data.examples[examples_id])['src']\r\n",
        "trg_example=vars(test_data.examples[examples_id])['trg']\r\n",
        "\r\n",
        "#translated\r\n",
        "trans_tokens,predict_attn=ModelTranlation(model,src_example,SRC,TRG,device)\r\n",
        "test_bleu=compute_bleu(model,test_data,SRC,TRG,device)\r\n",
        "print(f'Source sentences:{src_example}')\r\n",
        "print(f'translation Target sentences:{trg_example}')\r\n",
        "print(f'Machine translation:{trans_tokens}')\r\n",
        "print(f'Model belu score:{test_bleu}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}