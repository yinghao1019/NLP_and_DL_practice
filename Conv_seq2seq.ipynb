{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conv seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXEqgLn2lvPGqTsfGOZ/aM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yinghao1019/NLP_and_DL_practice/blob/master/Conv_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsDtl1ufydez"
      },
      "source": [
        "##Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb3InE_0xTP1"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn,optim\r\n",
        "import torch.nn.functional as F\r\n",
        "from torchtext.datasets import WMT14,Multi30k\r\n",
        "from torchtext.data import Field,BucketIterator,metrics\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot\r\n",
        "import spacy\r\n",
        "\r\n",
        "import os\r\n",
        "import tqdm\r\n",
        "import random\r\n",
        "import math\r\n",
        "import time"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f2_ywErAUXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea6c175-b9e1-4274-d885-df7206282858"
      },
      "source": [
        "#set random seed to fixed random number\r\n",
        "Random_SEED=1234\r\n",
        "random.seed(Random_SEED)\r\n",
        "np.random.seed(Random_SEED)\r\n",
        "torch.random.manual_seed(Random_SEED)\r\n",
        "torch.cuda.manual_seed_all(Random_SEED)\r\n",
        "if torch.backends.cudnn.is_available():\r\n",
        "  torch.backends.cudnn.deterministic=True\r\n",
        "#set gpu device\r\n",
        "device=torch.device('cuda:0')\r\n",
        "spacy.require_gpu()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfEH43cPzc89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee75ace-c68d-4e49-b1cc-71c7a499a382"
      },
      "source": [
        "#load tokenizer Model\r\n",
        "de_tokenizer=spacy.load('de_core_news_sm')\r\n",
        "# fr_tokenizer=spacy.load('fr_core_news_sm')\r\n",
        "en_tokenizer=spacy.load('en_core_web_sm')\r\n",
        "\r\n",
        "#build tokenize process\r\n",
        "#English toknize process\r\n",
        "def en_tokenize(text):\r\n",
        "  return [token.text.lower() for token in en_tokenizer.tokenizer(text)]\r\n",
        "#Gereman toknize process\r\n",
        "def de_tokenize(text):\r\n",
        "  return [token.text.lower() for token in de_tokenizer.tokenizer(text)]\r\n",
        "#Franch toknize process\r\n",
        "def fr_tokenize(text):\r\n",
        "  return [token.text.lower() for token in fr_tokenizer.tokenizer(text)]\r\n",
        "\r\n",
        "#build Field,we will translate English-German & English-Franch\r\n",
        "TRG_en=Field(init_token='<sos>',eos_token='<eos>',batch_first=True,tokenize=en_tokenize)\r\n",
        "SRC_de=Field(init_token='<sos>',eos_token='<eos>',batch_first=True,tokenize=de_tokenize)\r\n",
        "# TRG_fr=Field(init_token='<sos>',eos_token='<eos>',tokenize=fr_tokenize)\r\n",
        "\r\n",
        "#build data\r\n",
        "train_data,val_data,test_data=Multi30k.splits(exts=('.de','.en'),fields=(SRC_de,TRG_en))\r\n",
        "print(f'Dataset size:{len(train_data)}\\n{len(val_data)}\\n{len(test_data)}\\n')\r\n",
        "\r\n",
        "#display data examples info.\r\n",
        "train_idx=random.choice(range(0,len(train_data)))\r\n",
        "train_src=train_data.examples[train_idx].src\r\n",
        "train_trg=train_data.examples[train_idx].trg\r\n",
        "print(f'train src_sents:{train_src}\\n train trg sents:{train_trg}')\r\n",
        "#Build vocabulary\r\n",
        "SRC_de.build_vocab(train_data,min_freq=2,specials=['<unk>','<pad>'])\r\n",
        "TRG_en.build_vocab(train_data,min_freq=2,specials=['<unk>','<pad>'])\r\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset size:29000\n",
            "1014\n",
            "1000\n",
            "\n",
            "train src_sents:['ein', 'actionfoto', 'bei', 'einem', 'roller-derby-spiel', 'mit', 'einer', 'spielerin', 'im', 'vordergrund', '.']\n",
            " train trg sents:['a', 'roller', 'derby', 'match', 'with', 'an', 'action', 'shot', 'of', 'a', 'derby', 'girl', 'in', 'the', 'foreground', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hma7A1rbMQMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce28cd4-c05c-4da0-9de4-fe1345f1317a"
      },
      "source": [
        "#build Data iterator with bs128\r\n",
        "Batch_size=128\r\n",
        "train_iter,val_iter,test_iter=BucketIterator.splits((train_data,val_data,test_data),batch_sizes=(Batch_size,Batch_size,Batch_size),device=device)\r\n",
        "print(TRG_en.vocab.stoi['<pad>'])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIqa9DOefgjR"
      },
      "source": [
        "##Encoder implementation\r\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/raw/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq1.png)\r\n",
        "圖片來源:  \r\n",
        "\r\n",
        "Encoder每層Layer的說明-  \r\n",
        "1.token abd position Embed:首先，我們會將input sentence的word idx轉換成word Embeddin，除此之外，為了加強每個token位置資訊的重要性，採用element-wise sum的方法將位置資訊的embedding加入至word Embedding.\r\n",
        "\r\n",
        "2.linear layer:隨後我們將組合後的Embedding vector的embed_dim轉換成hidden dim.  \r\n",
        "\r\n",
        "3.N*conv block:然後我們將轉後的Embedding vector餵至'conv block'後，藉由N個'conv block'會提取出word的重要資訊,此時output的vector就是conved vector.  \r\n",
        "\r\n",
        "4.linear_layer:將轉換後的embedding feature之hid dim轉換成原來的embed_dim，此步驟的目的為由於後續會與Word Embedding進行residual connection,所以在dim部分必須一致\r\n",
        "\r\n",
        "5.residul connection:將一開始該token的word Embedding與conved vector進行加總成為combined output\r\n",
        "\r\n",
        "####而在Conv block中演算流程如下圖:\r\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/raw/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq2.png)  \r\n",
        "圖片來源:\r\n",
        "\r\n",
        "假設當前是採用\\n*embed_dim的2d filter,以及1個block的情況下來說明運算流程:\r\n",
        "1.在Input 部分:採用前述合併postion與word資訊的Embedding  \r\n",
        "2.convolution layer:我們將使用n*Embedding dim的filter來extract feature_map，這邊要注意的是只能在seqLen那個方向上的dim進行移動，並且N的大小可以把它看作如同採用N-gram Model來extract Feature一樣。\r\n",
        "Note:由於我們希望通過layer後的input ,output seqLen一致，所以採用padding的方式來進行調整。由於output須通過GLU activation func.如果為了讓input vector與output vector的dim一致，須將filter的channel數量設成hidden_dim的2倍才能達成條件。  \r\n",
        "3.最後我們將output與Input在進行一次residual connection.  \r\n",
        "4.第二個block的input則是吃第一個block的output，後續的block也是這樣。  \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHBeXDiRea55"
      },
      "source": [
        "#build convolution block\r\n",
        "class cnnEncoder(nn.Module):\r\n",
        "  def __init__(self,input_dim,embed_dim,hid_dim,kernel_size,n_layers,max_length,drop_rate,device):\r\n",
        "    super(cnnEncoder,self).__init__()\r\n",
        "    #embed_layer attr\r\n",
        "    self.input_dim=input_dim\r\n",
        "    self.pos_dim=max_length\r\n",
        "    self.embed_dim=embed_dim\r\n",
        "    #conv block attr.\r\n",
        "    self.n_layers=n_layers\r\n",
        "    self.n_channels=hid_dim*2\r\n",
        "    self.hid_dim=hid_dim\r\n",
        "    self.kernel_size=kernel_size\r\n",
        "    self.padding=(self.kernel_size-1)//2\r\n",
        "    \r\n",
        "    #build layer\r\n",
        "    self.word_embed=nn.Embedding(input_dim,embed_dim)\r\n",
        "    self.pos_embed=nn.Embedding(max_length,embed_dim)\r\n",
        "    self.embed2hid=nn.Linear(embed_dim,hid_dim)\r\n",
        "    self.conv_blocks=nn.ModuleList([nn.Conv1d(self.hid_dim,\r\n",
        "                        self.n_channels,\r\n",
        "                        kernel_size=self.kernel_size,\r\n",
        "                        padding=self.padding)\r\n",
        "                    for _ in range(self.n_layers)])\r\n",
        "    self.hid2embed=nn.Linear(hid_dim,embed_dim)\r\n",
        "    self.device=device\r\n",
        "    self.dropout=nn.Dropout(drop_rate)\r\n",
        "    self.scale=torch.sqrt(torch.FloatTensor([0.5])).to(device)\r\n",
        "  def forward(self,src_tensors):\r\n",
        "    #create pos tensors\r\n",
        "    batch_size=src_tensors.size()[0]\r\n",
        "    src_len=src_tensors.size()[1]\r\n",
        "    pos_tensors=torch.arange(0,src_len).unsqueeze(0).repeat(batch_size,1).to(self.device)\r\n",
        "\r\n",
        "    #get pos and word token Embed\r\n",
        "    #src_tokenEmbeds=[Bs,seqlen,embed_dim],src_posEmbeds=[Bs,seqlen,embed_dim]\r\n",
        "    src_tokenEmbeds=self.word_embed(src_tensors)\r\n",
        "    src_posEmbeds=self.pos_embed(pos_tensors)\r\n",
        "\r\n",
        "    #elemwise sum and linear transform\r\n",
        "    #src_hids=[Bs,seqlen,hid_dim]\r\n",
        "    src_embeds=self.dropout(src_tokenEmbeds+src_posEmbeds)\r\n",
        "    src_hids=self.embed2hid(src_embeds)\r\n",
        "\r\n",
        "    #permute dim\r\n",
        "    #conv_inputs=[Bs,hid,seqlen]\r\n",
        "    conv_inputs=src_hids.permute(0,2,1)\r\n",
        "\r\n",
        "    #N convblock extract conved embed\r\n",
        "    for convb in self.conv_blocks:\r\n",
        "      #Fed into convolution filter\r\n",
        "      #conved_output=[Bs,2*hid_dim,seqlen]\r\n",
        "      conved_output=convb(self.dropout(conv_inputs))\r\n",
        "\r\n",
        "      #through glu activation,shape=[Bs,2*hid_dim,seqlen]\r\n",
        "      #Residual connection=[Bs,hid_dim,seqlen]\r\n",
        "      conved_output=(conv_inputs+F.glu(conved_output,dim=1))*self.scale\r\n",
        "      conv_inputs=conved_output\r\n",
        "\r\n",
        "    #tranform conved output hid_dim to Embed_dim\r\n",
        "    #conved_output=[Bs,seqLen,Embed_dim]\r\n",
        "    conved_output=self.hid2embed(conved_output.permute(0,2,1))\r\n",
        "\r\n",
        "    #Through Residual connect\r\n",
        "    #combined_output=[Bs,seqLen,Embed_dim]\r\n",
        "    combined_output=(conved_output+src_embeds)*self.scale\r\n",
        "    return conved_output,combined_output"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97iexsES4At9"
      },
      "source": [
        "Decoder 架構部分"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhJUBHMB83Jq"
      },
      "source": [
        "class cnnDecoder(nn.Module):\r\n",
        "  def __init__(self,output_dim,embed_dim,hid_dim,kernel_size,n_layers,max_length,drop_rate,trg_pad_idx,device):\r\n",
        "    super(cnnDecoder,self).__init__()\r\n",
        "    #set decoder attr\r\n",
        "    self.output_dim=output_dim\r\n",
        "    self.pos_dim=max_length\r\n",
        "    self.embed_dim=embed_dim\r\n",
        "    self.device=device\r\n",
        "    self.trg_pad_idx=trg_pad_idx\r\n",
        "    #set conv_block attr.\r\n",
        "    self.hid_dim=hid_dim\r\n",
        "    self.n_filters=hid_dim*2\r\n",
        "    self.kernel_size=kernel_size\r\n",
        "    self.n_layers=n_layers\r\n",
        "    self.padding=kernel_size-1\r\n",
        "\r\n",
        "    #build layer\r\n",
        "    self.word_embed=nn.Embedding(output_dim,embed_dim)\r\n",
        "    self.pos_embed=nn.Embedding(self.pos_dim,embed_dim)\r\n",
        "    self.embed2hid=nn.Linear(self.embed_dim,self.hid_dim)\r\n",
        "    self.hid2embed=nn.Linear(self.hid_dim,self.embed_dim)\r\n",
        "    self.attn_hid2embed=nn.Linear(self.hid_dim,self.embed_dim)\r\n",
        "    self.attn_embed2hid=nn.Linear(self.embed_dim,self.hid_dim)\r\n",
        "    self.conv_blocks=nn.ModuleList([nn.Conv1d(self.hid_dim,self.n_filters,kernel_size=self.kernel_size) for _ in range(self.n_layers)])\r\n",
        "    self.padding=kernel_size-1\r\n",
        "    self.dropout=nn.Dropout(drop_rate)\r\n",
        "    self.output_layer=nn.Linear(self.embed_dim,self.output_dim)\r\n",
        "    self.scale=torch.sqrt(torch.FloatTensor([0.5])).to(device)\r\n",
        "  def calculate_attn(self,hid_tensors,embed_tensors,encoder_conved,encoder_combined):\r\n",
        "    #hid_tensors=[Bs,hid_dim,trg_seqlen]\r\n",
        "    #embed_tensors=[Bs,trg_seqlen,embed_dim]\r\n",
        "    #encoder_conved=[Bs,src_seqlen,embed_dim]\r\n",
        "    #encoder_combined=[Bs,src_seqlen,embed_dim]\r\n",
        "\r\n",
        "    #residual connection\r\n",
        "    #attn_query=[Bs,trg_seqlen,embed_dim]\r\n",
        "    attn_query=(self.attn_hid2embed(hid_tensors.permute(0,2,1))+embed_tensors)*self.scale\r\n",
        "\r\n",
        "    #get attn weight\r\n",
        "    #attn_w=[Bs,trg_seqlen,src_seqlen]\r\n",
        "    attn_w=torch.matmul(attn_query,encoder_conved.permute(0,2,1))\r\n",
        "\r\n",
        "    #through softmax and weighted sum encoder_combined\r\n",
        "    #attn_tensors=[Bs,trg_seqlen,emed_dim]\r\n",
        "    attn_tensors=torch.matmul(F.softmax(attn_w,dim=2),encoder_combined)\r\n",
        "\r\n",
        "    #convert to hid_dim and permute\r\n",
        "    #attn_tensors=[Bs,hid_dim,seqlen]\r\n",
        "    attn_tensors=self.attn_embed2hid(attn_tensors).permute(0,2,1)\r\n",
        "\r\n",
        "    #apply residual connection with input_hid and attended_input\r\n",
        "    attn_tensors=(attn_tensors+hid_tensors)*self.scale\r\n",
        "    return attn_tensors,attn_w\r\n",
        "  def forward(self,trg_tensors,encoder_conved,encoder_combined):\r\n",
        "    #create pos embedding\r\n",
        "    bs=trg_tensors.size()[0]\r\n",
        "    seqlen=trg_tensors.size()[1]\r\n",
        "    pos_tensors=torch.arange(0,seqlen).repeat(bs,1).to(self.device)\r\n",
        "\r\n",
        "    #pos_embed=[bs,seqlen,Embed_dim],token_embed=[bs,seqlen,Embed_dim]\r\n",
        "    token_embed=self.word_embed(trg_tensors)\r\n",
        "    pos_embed=self.pos_embed(pos_tensors)\r\n",
        "\r\n",
        "    #element-wise sum,embed_tensors=[Bs,seqLen,embed_dim]\r\n",
        "    embed_tensors=self.dropout(token_embed+pos_embed)\r\n",
        "    #convert into hid dim and transpose,conv_input=[Bs,hid_dim,seqLen]\r\n",
        "    conv_inputs=self.embed2hid(embed_tensors).permute(0,2,1)\r\n",
        "    \r\n",
        "    for conv_b in self.conv_blocks:\r\n",
        "      #create conv pad to avoid cheat\r\n",
        "      #new conv_inputs=[Bs,hid_dim,(filter_size-1)+orig_seqlen]\r\n",
        "      conv_inputs=self.dropout(conv_inputs)\r\n",
        "      conv_pad=torch.zeros(bs,self.hid_dim,self.padding).fill_(self.trg_pad_idx).to(self.device)\r\n",
        "      conv_pad=torch.cat((conv_pad,conv_inputs),dim=2)\r\n",
        "\r\n",
        "      #conv_output=[Bs,hid_dim*2,seqLen]\r\n",
        "      conv_outputs=conv_b(conv_pad)\r\n",
        "      #through GLU and transpose=[Bs,seqlen,hid_dim]\r\n",
        "      conv_outputs=F.glu(conv_outputs,dim=1)\r\n",
        "      #calculate attention=[Bs,hid_dim,seqlen]\r\n",
        "      conv_outputs,conv_attnW=self.calculate_attn(conv_outputs,embed_tensors,encoder_conved,encoder_combined)\r\n",
        "\r\n",
        "      #Residual connection attn output & conv_inputs\r\n",
        "      #conv_outputs=[Bs,hid_dim,seqLen]\r\n",
        "      conv_outputs=(conv_outputs+conv_inputs)*self.scale\r\n",
        "      conv_inputs=conv_outputs\r\n",
        "\r\n",
        "    #transform hid_dim to embed_dim=[Bs,seqlen,embed_dim]\r\n",
        "    embed_outputs=self.hid2embed(conv_outputs.permute(0,2,1))\r\n",
        "\r\n",
        "    #output each token logitics=[Bs,seqlen,output_dim]\r\n",
        "    #conv_attnW=[Bs,]\r\n",
        "    return self.output_layer(embed_outputs),conv_attnW"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVGMdhwDnqIy"
      },
      "source": [
        "Seq2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11UOd1Vlnpt8"
      },
      "source": [
        "class cnn_Seq2Seq(nn.Module):\r\n",
        "  def __init__(self,encoder,decoder):\r\n",
        "    super(cnn_Seq2Seq,self).__init__()\r\n",
        "    self.encoder=encoder\r\n",
        "    self.decoder=decoder\r\n",
        "  def forward(self,src_tensors,trg_tensors):\r\n",
        "    #src_tensors=[Bs,src_seqlen]\r\n",
        "    #trg_tensors=[Bs,trg_seqlen-1]\r\n",
        "\r\n",
        "    #encoding src_tensor\r\n",
        "    #conved_output=[Bs,seqlen,Embed_dim],conv_combined=[Bs,seqlen,Embed_dim]\r\n",
        "    conved_output,conv_combined=self.encoder(src_tensors)\r\n",
        "    #decoder conved_output\r\n",
        "    output_logitics,attn_w=self.decoder(trg_tensors,conved_output,conv_combined)\r\n",
        "    return output_logitics,attn_w"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "680KlyG_bhzE"
      },
      "source": [
        "Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9p6zVATyVdr"
      },
      "source": [
        "#set Encoder & Decoder hyperparameters\r\n",
        "Input_dim=len(SRC_de.vocab)\r\n",
        "\r\n",
        "Output_dim=len(TRG_en.vocab)\r\n",
        "Embed_dim=256\r\n",
        "hid_dim=512 #so filter size=1024\r\n",
        "Encoder_layers=10\r\n",
        "Decoder_layers=10\r\n",
        "Enc_kernel_size=3\r\n",
        "Dec_kernel_size=3\r\n",
        "Max_length=100\r\n",
        "Enc_dropout=0.25\r\n",
        "Dec_dropout=0.25\r\n",
        "TRG_pad_idx=TRG_en.vocab.stoi['<pad>']\r\n",
        "model_params={'encoder':{'input_dim':Input_dim,'kernel_size':Enc_kernel_size,'n_layers':Encoder_layers,'drop_rate':Enc_dropout},\r\n",
        "       'decoder':{'output_dim':Output_dim,'kernel_size':Dec_kernel_size,'n_layers':Decoder_layers,'drop_rate':Dec_dropout,'trg_pad_idx':TRG_pad_idx},\r\n",
        "       'common':{'embed_dim':Embed_dim,'hid_dim':hid_dim,'max_length':Max_length,'device':device}}\r\n",
        "model_dir='./model'\r\n",
        "model_path='s2s_model.pt'"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sekj3wbE4dcm"
      },
      "source": [
        "def count_parameters(m):\r\n",
        "  return sum([p.numel() for p in m.parameters() if p.requires_grad])"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygr9Nh9U2NXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8400917-ab3a-493e-a1a6-aae637dcca88"
      },
      "source": [
        "encoder_model=cnnEncoder(**model_params['encoder'],**model_params['common'])\r\n",
        "decoder_model=cnnDecoder(**model_params['decoder'],**model_params['common'])\r\n",
        "s2s_model=cnn_Seq2Seq(encoder_model,decoder_model).to(device)#put model to GPU\r\n",
        "#compute Model trainable parameters\r\n",
        "print(f'Model trainable params num:{count_parameters(s2s_model)}')\r\n",
        "print(f'Model artictcure:{s2s_model}')\r\n",
        "optimizer=optim.Adam(s2s_model.parameters())\r\n",
        "criterion=nn.CrossEntropyLoss(ignore_index=TRG_pad_idx)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model trainable params num:37351685\n",
            "Model artictcure:cnn_Seq2Seq(\n",
            "  (encoder): cnnEncoder(\n",
            "    (word_embed): Embedding(7855, 256)\n",
            "    (pos_embed): Embedding(100, 256)\n",
            "    (embed2hid): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (conv_blocks): ModuleList(\n",
            "      (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (8): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (9): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    )\n",
            "    (hid2embed): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (dropout): Dropout(p=0.25, inplace=False)\n",
            "  )\n",
            "  (decoder): cnnDecoder(\n",
            "    (word_embed): Embedding(5893, 256)\n",
            "    (pos_embed): Embedding(100, 256)\n",
            "    (embed2hid): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (hid2embed): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_hid2embed): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_embed2hid): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (conv_blocks): ModuleList(\n",
            "      (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
            "      (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
            "      (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
            "      (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
            "      (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
            "      (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
            "      (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
            "      (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
            "      (8): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
            "      (9): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
            "    )\n",
            "    (dropout): Dropout(p=0.25, inplace=False)\n",
            "    (output_layer): Linear(in_features=256, out_features=5893, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aas2Ram9Odje"
      },
      "source": [
        "#define model training pipeline\r\n",
        "def train_model(model,train_iter,criterion,optimizer,device,clip):\r\n",
        "  train_loss=0\r\n",
        "  model.train()\r\n",
        "  for bs in train_iter:\r\n",
        "    if next(model.parameters()).is_cuda:\r\n",
        "      src_tensors=bs.src.to(device)\r\n",
        "      trg_tensors=bs.trg.to(device)\r\n",
        "    else:\r\n",
        "      src_tensors=bs.src\r\n",
        "      trg_tensors=bs.trg\r\n",
        "    output_trg,_=model(src_tensors,trg_tensors[:,:-1])\r\n",
        "    #compute loss\r\n",
        "    output_trg=output_trg.reshape(-1,model.decoder.output_dim)\r\n",
        "    labels_trg=trg_tensors[:,1:].reshape(-1)\r\n",
        "    loss=criterion(output_trg,labels_trg)\r\n",
        "    train_loss+=loss.item()\r\n",
        "\r\n",
        "    #compute gradient & clipping\r\n",
        "    loss.backward()\r\n",
        "    nn.utils.clip_grad_norm_(model.parameters(),clip)\r\n",
        "\r\n",
        "    #update model weight\r\n",
        "    optimizer.step()\r\n",
        "    optimizer.zero_grad()\r\n",
        "  return train_loss/len(train_iter)\r\n",
        "#define model testing pipeline\r\n",
        "def evaluate_model(model,val_iter,criterion,device):\r\n",
        "  total_loss=0\r\n",
        "  model.eval()\r\n",
        "  with torch.no_grad():\r\n",
        "    for bs in val_iter:\r\n",
        "      if next(model.parameters()).is_cuda:\r\n",
        "        src_tensors=bs.src.to(device)\r\n",
        "        trg_tensors=bs.trg.to(device)\r\n",
        "      else:\r\n",
        "        src_tensors=bs.src\r\n",
        "        trg_tensors=bs.trg\r\n",
        "      trg_outputs,_=model(src_tensors,trg_tensors[:,:-1])\r\n",
        "      #compute loss\r\n",
        "      trg_outputs=trg_outputs.reshape(-1,model.decoder.output_dim)\r\n",
        "      trg_labels=trg_tensors[:,1:].reshape(-1)\r\n",
        "      loss=criterion(trg_outputs,trg_labels)\r\n",
        "      total_loss+=loss.item()\r\n",
        "  return total_loss/len(val_iter)\r\n",
        "def save_modelCkp(model,optimizer,model_params,epochs,model_dir,model_path):\r\n",
        "  if not os.path.exists(model_dir):\r\n",
        "    print('Model dir not exists')\r\n",
        "    os.mkdir(model_dir)\r\n",
        "    print(f'Already create directory:{model_dir}!')\r\n",
        "  save_path=os.path.join(model_dir,model_path)\r\n",
        "  torch.save({\r\n",
        "        'model':model.state_dict(),'optimzier':optimizer.state_dict(),\r\n",
        "        'model_params':model_params,'epochs':epochs\r\n",
        "        },save_path)\r\n",
        "  print(f'Model checkpoint already saved to{save_path}!')\r\n",
        "def load_modelCkp(model_dir,model_path):\r\n",
        "  save_path=os.path.join(model_dir,model_path)\r\n",
        "  #detect dir_path\r\n",
        "  if os.path.exits(model_dir):\r\n",
        "    print('Model dir exists...')\r\n",
        "  else:\r\n",
        "    raise FileNotFoundError('Model dir not exists')\r\n",
        "\r\n",
        "  try:\r\n",
        "    ck_point=torch.load(save_path)\r\n",
        "    print('Successful load Model info.')\r\n",
        "    return ck_point\r\n",
        "  except FileNotFoundError:\r\n",
        "    print(\"Save path don't exists\")"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz47LFljrc2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2c141f-8de6-4f58-c0d2-a1259af9240a"
      },
      "source": [
        "train_epochs=15\r\n",
        "grad_clip=0.1\r\n",
        "epochs_progress=tqdm.trange(train_epochs)\r\n",
        "save_everyEpochs=3\r\n",
        "for ep in epochs_progress:\r\n",
        "  #train Model\r\n",
        "  start_time=time.time()\r\n",
        "  train_loss=train_model(s2s_model,train_iter,criterion,optimizer,device,grad_clip)\r\n",
        "  end_time=time.time()\r\n",
        "  epochs_loss=evaluate_model(s2s_model,val_iter,criterion,device)#compute val loss\r\n",
        "\r\n",
        "  #save Model checkpoints in every 3 epochs and final epochs\r\n",
        "  if (ep+1)%save_everyEpochs==0 or (ep+1)%save_everyEpochs==train_epochs:\r\n",
        "    save_modelCkp(s2s_model,optimizer,model_params,ep+1,model_dir,model_path)\r\n",
        "  \r\n",
        "  #display Training information\r\n",
        "  print(f'[{ep+1}/{train_epochs}] train_loss:{train_loss} \\t eval_loss:{epochs_loss} cost_time:{end_time-start_time}s')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "\n",
            "  7%|▋         | 1/15 [01:00<14:10, 60.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1/15] train_loss:5.593705547013472 \t eval_loss:3.853549540042877 cost_time:60.29278635978699s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 13%|█▎        | 2/15 [02:04<13:22, 61.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2/15] train_loss:3.540686007638335 \t eval_loss:2.7958647310733795 cost_time:63.46308350563049s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 20%|██        | 3/15 [03:09<12:33, 62.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model checkpoint already saved to./model/s2s_model.pt!\n",
            "[3/15] train_loss:2.789023985421605 \t eval_loss:2.302263021469116 cost_time:63.15321946144104s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 27%|██▋       | 4/15 [04:13<11:34, 63.10s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[4/15] train_loss:2.421034505188728 \t eval_loss:2.0697994977235794 cost_time:63.41914439201355s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 33%|███▎      | 5/15 [05:17<10:32, 63.29s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[5/15] train_loss:2.203066254502351 \t eval_loss:1.9650320261716843 cost_time:63.25152349472046s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 40%|████      | 6/15 [06:23<09:35, 63.99s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model checkpoint already saved to./model/s2s_model.pt!\n",
            "[6/15] train_loss:2.0616136369201055 \t eval_loss:1.9085756987333298 cost_time:63.59196972846985s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 47%|████▋     | 7/15 [07:26<08:31, 63.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[7/15] train_loss:1.9559736236076524 \t eval_loss:1.8502509742975235 cost_time:63.21267509460449s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 53%|█████▎    | 8/15 [08:30<07:26, 63.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[8/15] train_loss:1.8709784856451765 \t eval_loss:1.8321518748998642 cost_time:63.12448811531067s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 60%|██████    | 9/15 [09:35<06:25, 64.27s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model checkpoint already saved to./model/s2s_model.pt!\n",
            "[9/15] train_loss:1.8059868077349557 \t eval_loss:1.8221572786569595 cost_time:63.32690167427063s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 67%|██████▋   | 10/15 [10:39<05:20, 64.07s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10/15] train_loss:1.7476308718651927 \t eval_loss:1.801610380411148 cost_time:63.12400960922241s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 73%|███████▎  | 11/15 [11:43<04:16, 64.03s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[11/15] train_loss:1.6993651374321153 \t eval_loss:1.7833548039197922 cost_time:63.436952352523804s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 80%|████████  | 12/15 [12:48<03:13, 64.37s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model checkpoint already saved to./model/s2s_model.pt!\n",
            "[12/15] train_loss:1.6537788281881862 \t eval_loss:1.7976618558168411 cost_time:63.155280113220215s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 87%|████████▋ | 13/15 [13:51<02:08, 64.11s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[13/15] train_loss:1.6192423520109203 \t eval_loss:1.7762237936258316 cost_time:63.01210379600525s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 93%|█████████▎| 14/15 [14:55<01:03, 63.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[14/15] train_loss:1.5832782500640936 \t eval_loss:1.7591708451509476 cost_time:62.95311903953552s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 15/15 [16:00<00:00, 64.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model checkpoint already saved to./model/s2s_model.pt!\n",
            "[15/15] train_loss:1.550870656967163 \t eval_loss:1.7740721553564072 cost_time:63.148723125457764s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycenoUyiK6ML"
      },
      "source": [
        "Inference stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ypfrgHyK3c-"
      },
      "source": [
        "def translate_sents(model,sentence,src_Field,trg_field,max_len,device):\r\n",
        "  model.eval()\r\n",
        "  #text process\r\n",
        "  if isinstance(sentence,str):\r\n",
        "    en_tokenizer=spacy.load('en_core_web_sm')\r\n",
        "    translated=['<sos>']+[token.text.lower() for t in en_tokenizer(sentence)]+['<eos>']\r\n",
        "  else:\r\n",
        "    translated=['<sos>']+[t.lower() for t in sentence]+['<eos>']\r\n",
        "  print(f'After tokenized:{translated}')\r\n",
        "  #convert to word indx\r\n",
        "  token_idx=src_Field.numericalize([translated],device)\r\n",
        "  print(f'transform word index:{token_idx}')\r\n",
        "  #create source sents Tensor\r\n",
        "  src_tensors=token_idx.to(device)\r\n",
        "\r\n",
        "  with torch.no_grad():\r\n",
        "    #encoding source sents\r\n",
        "    conved_output,combined_output=model.encoder(src_tensors)\r\n",
        "\r\n",
        "  trg_idx=[trg_field.vocab.stoi['<sos>']]\r\n",
        "  i=0\r\n",
        "  while i<max_len:\r\n",
        "    model_output=torch.LongTensor(trg_idx).unsqueeze(0).to(device)\r\n",
        "    #outputs=[1,seqlen,output_dim]\r\n",
        "    output_logitics,attn_weights=model.decoder(model_output,conved_output,combined_output)\r\n",
        "    #get predict word idx\r\n",
        "    predict_idx=F.softmax(output_logitics,dim=2).argmax(2)[:,-1].item()\r\n",
        "\r\n",
        "    #determined predict token id\r\n",
        "    if predict_idx==trg_field.vocab.stoi['<eos>']:\r\n",
        "      break\r\n",
        "    trg_idx.append(predict_idx)\r\n",
        "    i+=1\r\n",
        "  #convert word idx into string\r\n",
        "  translated_sents=[trg_field.vocab.itos[idx] for idx in trg_idx]\r\n",
        "  \r\n",
        "  return translated_sents[1:],attn_weights"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWkAFr7Ac5Fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "158ef678-3a91-4728-e5f8-37f190c23310"
      },
      "source": [
        "#Translate test example\r\n",
        "test_idx=random.choice(range(len(test_data)))\r\n",
        "test_example=test_data.examples[test_idx]\r\n",
        "test_src=test_example.src\r\n",
        "test_trg=test_example.trg\r\n",
        "\r\n",
        "#translated source sent\r\n",
        "translated_trg,attn_w=translate_sents(s2s_model,test_src,SRC_de,TRG_en,100,device)\r\n",
        "print(f'Translated source sentence:{test_src}\\n')\r\n",
        "print(f'Actual target sentence:{test_trg}\\n')\r\n",
        "print(f\"model translated target sentence:{' '.join(translated_trg)}\\n\")\r\n",
        "print(attn_w)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After tokenized:['<sos>', 'eine', 'afrikanische', 'familie', 'steht', 'vor', 'ein', 'paar', 'provisorischen', 'behausungen', '.', '<eos>']\n",
            "transform word index:tensor([[   2,    8, 1088,  323,   29,   27,    5,  116,    0,    0,    4,    3]])\n",
            "Translated source sentence:['eine', 'afrikanische', 'familie', 'steht', 'vor', 'ein', 'paar', 'provisorischen', 'behausungen', '.']\n",
            "\n",
            "Actual target sentence:['an', 'african', 'family', 'are', 'standing', 'in', 'front', 'of', 'some', 'makeshift', 'houses', '.']\n",
            "\n",
            "model translated target sentence:a african family stands in front of some <unk> <unk> .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZcJ2P4tix8z"
      },
      "source": [
        "檢視test eaxmple Target sents關注source sents的注意力權重，顏色越白者代表關注度越高"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEtcGb8pixhk"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib as mpl\r\n",
        "#get tick labels location\r\n",
        "xticks_loc=range(len(test_src))\r\n",
        "yticks_loc=range(len(translated_trg))\r\n",
        "#create fig\r\n",
        "fig=plt.figure(figsize=(10,8))\r\n",
        "ax=fig.add_subplot(1,1,1)\r\n",
        "ax.imshow()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}